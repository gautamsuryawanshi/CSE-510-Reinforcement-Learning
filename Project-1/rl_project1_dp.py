# -*- coding: utf-8 -*-
"""RL-Project1-DP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J84WXIFp_JlAH4RUkzcb_6_f2TW2bHzf
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces

class GridEnvironment(gym.Env):
    metadata = { 'render.modes': [] }
    
    def __init__(self):
        self.states = []
        self.gamma = 1
        self.policy=0.25
        self.transProb=1
        self.reward=[0,-1,1]
        self.actions = ["up", "down", "left", "right"]
        self.values = np.zeros((3,3))
        self.values_new=np.zeros((3,3))

        self.actions=[]

        self.observation_space = spaces.Discrete(9)
        self.action_space = spaces.Discrete(4)
        self.max_timesteps = 5
        
    
        

    def val_func(self,i,j,pos):
      if(pos[0]<0 or pos[0]>2 or pos[1]<0 or pos[1]> 2):
        return self.values[i][j]
      else:
        return self.values[tuple(pos)]

    def get_reward(self,i,j):
      if(i == 0 and j == 0):
        return 0
      elif(i==2 and j==2):
        return 2
      else:
        return 1
    
    def calc_func(self,v1,v2,v3,v4,r1,r2,r3,r4):
      
      v=self.policy*(self.transProb*((r1+self.gamma*v1)+(r2+self.gamma*v2)+(r3+self.gamma*v3)+(r4+self.gamma*v4)))
      return v

    def take_action(self,i,j):
      up = [i-1,j]
      v1 = self.val_func(i,j,up) 
      r1 = self.get_reward(i,j)
  

      down = [i+1,j]
      v2 = self.val_func(i,j,down) 
      r2=self.get_reward(i,j)

      right = [i,j+1]
      v3 = self.val_func(i,j,right)
      r3=self.get_reward(i,j) 

      left = [j,j-1]
      v4 = self.val_func(i,j,left)
      r4=self.get_reward(i,j)

 
     
      value=self.calc_func(v1,v2,v3,v4,r1,r2,r3,r4)
      return value

    def step_func(self,i, j):
      up=self.val_func(i,j,[i-1,j])
      down=self.val_func(i,j,[i+1,j])
      right =self.val_func(i,j,[i,j-1])
      left=self.val_func(i,j,[i,j+1])
      max_value=np.max([up,left,right,down])

      if max_value==up:
        self.agent_pos=[i-1,j]
        self.actions.append(1)
      elif max_value==down:
        self.agent_pos=[i+1,j]
        self.actions.append(0)
      elif max_value==right:
        self.agent_pos=[i,j-1]
        self.actions.append(2)
      elif max_value == left:
        self.agent_pos=[i,j+1]
        self.actions.append(3)
      

      return self.agent_pos

     


    def iterate_func(self,val):
      for k in range(val):
        for i in range(3):
          for j in range(3):
            if(i==0 and j==0):
              print(" ")
            else:
             self.values_new[i,j]=self.take_action(i,j)
        print("************")
        print(self.values_new)
        self.values=self.values_new
        self.values_new=np.zeros((3,3))


    def render(self,k):
      self.iterate_func(k)
      agent_move_dir=[0,0]
      for a in range(4):
        pos=self.step_func(agent_move_dir[0],agent_move_dir[1])
        print("The agent positions are "+str(pos))
        agent_move_dir=[pos[0],pos[1]]
        grid = np.zeros((3,3))
        grid[tuple(agent_move_dir)] = 1
        plt.imshow(grid)
        plt.pause(1)



def main():
    env = GridEnvironment()
    env.render(3)
    print(env.actions)


if __name__=="__main__":
    main()




