# -*- coding: utf-8 -*-
"""Rl-Project-1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qiXynoaUF2HDnY2zByTAT-aMka_0DIIf
"""

import numpy as np
import matplotlib.pyplot as plt
import gym
from gym import spaces

class GridEnvironment(gym.Env):
    metadata = { 'render.modes': [] }
    
    def __init__(self):
        self.observation_space = spaces.Discrete(9)
        self.action_space = spaces.Discrete(4)
        self.max_timesteps = 5
        
    def reset(self):
        self.timestep = 0
        self.agent_pos = [0, 0]
        self.goal_pos = [2, 2]
        self.death = [2, 0]
        self.state = np.zeros((3,3))
        self.state[tuple(self.agent_pos)] = 1
        self.state[tuple(self.goal_pos)] = 0.5
        self.state[tuple(self.death)] = 0.2
        observation = self.state.flatten()
        return observation
    
    def step(self, action):
        # self.state = np.random.choice(self.observation_space.n)
        if action == 0:
          self.agent_pos[0] += 1
        if action == 1:
          self.agent_pos[0] -= 1
        if action == 2:
          self.agent_pos[1] += 1
        if action == 3:
          self.agent_pos[1] -= 1
          
        self.agent_pos = np.clip(self.agent_pos, 0, 2)
        self.state = np.zeros((3,3))
        self.state[tuple(self.agent_pos)] = 1
        self.state[tuple(self.goal_pos)] = 0.5
        self.state[tuple(self.death)] = 0.2
        observation = self.state.flatten()
        
        reward = 0
        if (self.agent_pos == self.goal_pos).all():
          reward = 1
        if (self.agent_pos == self.death).all():
          reward = -1
        
        self.timestep += 1
        done = True if self.timestep >= self.max_timesteps else False
        info = {}
        
        return observation, reward, done, info
        
    def render(self):
        plt.imshow(self.state)



def main():
    env = GridEnvironment()
    env.reset()
    action = 2
    observation, reward, done, info = env.step(action)
    env.render()
    print('Reward:', reward)    
        #Deterministic Env
    # 0 - down
    # 1 - up
    # 2 - right
    # 3 - left
    env = GridEnvironment()
    env.reset()
    actions=[2,2,0,0]
    for action in actions:
      observation, reward, done, info = env.step(action)
      env.render()
      print('Reward:', reward)
      
if __name__=="__main__":
    main()

        



